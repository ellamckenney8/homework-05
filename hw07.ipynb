{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ellamckenney8/homework-05/blob/main/hw07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vHN4w7fQuEI"
      },
      "source": [
        "# HW7\n",
        "\n",
        "\n",
        "In this week's homework we will work with some questions of probability generally, as well as with the High Time Resolution Universe (HTRU) Pulsar Survey we have seen in lecture.\n",
        "\n",
        "> The High Time Resolution Universe (HTRU) is an all-sky survey for pulsars and radio transients at a frequency of 1400 MHz.\n",
        ">\n",
        "> The Southern Hemisphere is being observed with the Parkes Multi-Beam system, the Northern Hemisphere is being observed with the Effelsberg 7-beam system. It is expected that the survey sensitivity will be similar for  both hemispheres.\n",
        ">\n",
        "> These surveys will have a much higher frequency and time resolution than previous surveys like the Parkes Multi-Beam Survey. Because of this, they will suffer much less from dispersive smearing and will therefore be able to detect a much larger number of millisecond pulsars, particularly near the Galactic plane, where we expect the most exciting binary systems and a much larger population of pulsars. These surveys will likely find hundreds of millisecond pulsars, which are great laboratories for the study of fundamental physics, gravitational astronomy and astrophysics in general. Furthermore, the Effelsberg part of the survey (HTRU-North) will survey the whole Northern Hemisphere for the first time in 30 years,allowing a new high-sensitive view onto this part of the sky. The Northern Hemisphere surveys have a great advantage that new discoveries can be followed up with a large variety of radio telescopes, which maximizes scientific output.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qxv7OL_LQuEM"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "\n",
        "pd.options.mode.chained_assignment = None\n",
        "np.random.seed(206206)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aYMBlZYjQw4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBpMmmt1QuEN"
      },
      "source": [
        "htru = pd.read_csv(\"https://github.com/UM-Data-Science-101/homework-07/raw/refs/heads/main/HTRU_2.csv\", names =\n",
        "  [\"profile_mean\", \"profile_sd\",\n",
        "  \"profile_excess_kurtosis\", \"profile_skewness\",\n",
        "  \"snr_mean\",\"snr_sd\", \"snr_excess_kurtosis\", \"snr_skewness\",\n",
        "  \"pulsar\"])\n",
        "print(htru.shape)\n",
        "htru.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvNGBtvSQuEO"
      },
      "source": [
        "Recall that **profile** refers to the distribution of radio frequencies recieved at each location in space being measured by the radio telescope, and **snr** refers to the distribution of \"signal to noise ratios\" (a measure of signal strength) at each location. We don't see the full distribution, but we get summary measures in the form of means, standard deviations, coefficients of skewness and \"excess kurtosis\" (which we haven't talked about but means how common are extreme values (either small or large)).\n",
        "\n",
        "We also know whether an observation is a [pulsar](https://en.wikipedia.org/wiki/Pulsar) or not.\n",
        "\n",
        "## Question 1: Getting to Know the Data\n",
        "\n",
        "### Q1.a\n",
        "\n",
        "Create 4 histogram plots for the marginal distributions of\n",
        "\n",
        "- `profile_mean`\n",
        "- `profile_sd`\n",
        "- `snr_mean`\n",
        "- `snr_sd`\n",
        "\n",
        "For each plot, briefly note typical values and if you think it exhibits skew (if so, in which direction)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlhJ0WQBQuEO"
      },
      "source": [
        "# profile_mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2Zhtvm3QuEO"
      },
      "source": [
        "# profile_sd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zu9zFGKZQuEO"
      },
      "source": [
        "# snr_mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Smc9ybfAQuEP"
      },
      "source": [
        "# snr_sd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ry6Vp-HQuEP"
      },
      "source": [
        "### Q1.b\n",
        "\n",
        "Verify your results for the previous problem by computing means, medians, and coefficients of skewness for each of these 4 variables. Use the [`aggregate` method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.aggregate.html) (see the examples at the bottom of this link in particular)  applied to table with only these four columns to compute your results.\n",
        "\n",
        "Explain why your statements about skew are either supported or not supported by the computed coefficients of skewness."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQTUQrbiQuEP"
      },
      "source": [
        "# answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7wy9a3NQuEQ"
      },
      "source": [
        "## Q1.c\n",
        "\n",
        "At this point, you might be thinking that we should apply transformations to these variables that show extreme skewness. If we were just going to use them by themselves, this would be a good approach, but since we are about to consider differences between pulsars and non-pulsars, it is important to be sure that these skewness issues persist for the **conditional distributions** given pulsar status.\n",
        "\n",
        "For the most extreme skewness from above, recreate the histogram adding `puslar` as a `hue` visual variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6WT_-cUQuEQ"
      },
      "source": [
        "# plot with hue"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puXCavDlQuEQ"
      },
      "source": [
        "Compute the **conditional coefficient of skewness given pulsar status** (hint: \"conditional\" means \"groupby\" when the thing we are conditioning on is nominal). Does the transformation seem necessary now?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jiDAGx8QuEQ"
      },
      "source": [
        "# conditional skewness"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ygGRgZTQuEQ"
      },
      "source": [
        "## Q1.d\n",
        "\n",
        "In situations like these, it can sometimes be better to see if there a few observations that are strong skewing the data. Create a table that is just non-pulsars (where `\"pulsar\"` is 0). Compute the 0.9, 0.95, 0.99 quantiles for the most skewed variable we investigated in the previous part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahtMpFaZQuEQ"
      },
      "source": [
        "# quantiles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqTqYvPPQuEQ"
      },
      "source": [
        "Select one of these values and discard all observations in the htru table that exceed this value on the most extreme variable and are also non-pulsars. Call the result `htru_clean` (we will use it in the next section). Recompute the conditional coefficient of skewness to verify this helped. (Hint: remember that `&` means \"and\", `|` means \"or\" and `~` means \"not\" when writing comparisons of pandas columns)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-u0vsb6QuER"
      },
      "source": [
        "# htru_clean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saVFdRcYQuER"
      },
      "source": [
        "## Question 2: Differences of Pulsars and Non-Pulsars\n",
        "\n",
        "This problem will use the `htru_clean` table from the previous problem. If you are having problems creating that table, you may use the `htru` table but will receive a 2 point deduction on your final score.\n",
        "\n",
        "\n",
        "### Q2.a\n",
        "\n",
        "For pulsars and non-pulsars, compute conditional means on the 4 variables of interest from the last section (\"pulsar_mean\", \"pulsar_sd\", \"snr_mean\", \"snr_sd\") *simulateously* such that\n",
        "you get two rows (one for pulsars, one for non-pulsars) with a column for each  variable's sample mean. Then use the .transpose() method to swap the rows and columns so that you get two columns and 4 rows. Save this to a variable (I will call mine `pnp_means`).\n",
        "\n",
        "Repeat  with the `.var()` method, saving to a different variable (I will call mine `pnp_vars`).\n",
        "\n",
        "Repeat again with the `.count()` to get the sample sizes (I will call mine `pnp_ns`). In this particular case all the values will be the same in each column, but this is a smart move in case there is missing data for some variables but not others.\n",
        "\n",
        "Print out all tables for verification by the graders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIkHS_s0QuER"
      },
      "source": [
        "# conditional means, variances, counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GxF9JAfQuER"
      },
      "source": [
        "### Q2.b\n",
        "\n",
        "For each variable, you can compute the difference of means simultaneously with `pnp_means[1] - pnp_means[0]`.\n",
        "\n",
        "For each variable you can compute the **pooled standard deviation** by\n",
        "\n",
        "1. Multiply the `pnp_vars` by `pnp_ns`.\n",
        "2. Sum the that result using `.sum(axis = 1)`\n",
        "3. Divide that value by the `.sum(axis = 1)` of the `pnp_ns`.\n",
        "3. Use `np.sqrt` on the result.\n",
        "\n",
        "Divide the differences by the pooled standard deviations to get effect size differences comparing pulsars and non-pulsars. Interpret these results using the able of effect sizes and clearly state which group has the large value for each variable.\n",
        "\n",
        "Effect size magnitude )\n",
        "* Very small: 0 - 0.01\n",
        "* Small: 0.1 - 0.20\n",
        "* Medium: 0.2 - \t0.50\n",
        "* Large: 0.5 - \t0.80\n",
        "* Very large: 0.8 0- \t1.20\n",
        "* Huge: 1.2 - \t2.0+"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1Pta6AzQuER"
      },
      "source": [
        "# effect sizes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYzhfvoKQuER"
      },
      "source": [
        "## Question 3: Probability of Being a Pulsar\n",
        "\n",
        "For this question, we will continue to use the `htru_clean`. If you used `htru` on the previous problem, there will be no additional penalty for using `htru` on this problem.\n",
        "\n",
        "### Q3.a\n",
        "\n",
        "Recall that the `pulsar` column is coded as \"1\" is a pulsar, \"0\" is a non-pulsar. What is the proportion of observations are pulsars? Compute this value using a single method call on the `htru_clean[\"pulsar\"]` column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GE2Zk7EKQuER"
      },
      "source": [
        "# proportion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wd_vHWp9QuER"
      },
      "source": [
        "## Q3.b\n",
        "\n",
        "Using the `pd.qcut` function, create a new column in `htru_clean` that represents which *decile of `profile_mean`* each observation falls into (i.e., make the first argument the `\"profile_mean\"` column and the second argument 10). Save the result to a new column (I will call mine `pm_decile`).\n",
        "\n",
        "Compute the **conditional proportion of observations that are pulsars** for each level of `pm_decile` (this is a time when `.reset_index()` will be useful).\n",
        "\n",
        "Use the supplied `get_mid` function to turn the `pm_decile` column in your result into a value that represents the mid-point of the intervals we got from `qcut`, saving the result as a new column in the able (call it `pm_mid`).\n",
        "\n",
        "\n",
        "Plot `pm_mid` on the horizontal axis and the proportion of pulsars on the veritcal axis as a line plot. Comment on what you see. What happens to the percentage of objects that are pulsars as the typical radio frequency increases.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4jlys3dQuER"
      },
      "source": [
        "# solution"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1qL6Q7xQuER"
      },
      "source": [
        "### Q3.c\n",
        "\n",
        "Of course, it would be nice to avoid having to break up `profile_mean` into a set of discrete groups. Instead, we can perform linear regression with `pulsar` as $Y$ and `profile_mean` as $X$. Recall that linear regression is attempting to model the **conditional mean** of $Y$ given $X$. Since a proportion is a special case of a mean when $Y$ is 0 or 1, and proportions are *empirical probabilities*, this approach is called a \"linear probability model\".\n",
        "\n",
        "Compute the linear regression regression with `pulsar` as $Y$ and `profile_mean` as $X$. Do not forget to add the constant term. Print out the result.\n",
        "\n",
        "According to this model:\n",
        "- What does the sign of the coefficient indicate?\n",
        "- What is the probability that an observation with a `profile_mean` 120 would be a pulsar?\n",
        "- What is the probability that an observation with a `profile_mean`of 60 would be a pulsar?\n",
        "- For two groups of observations that differ by 100 units on `pulsar_mean`, what would be the difference in proportions of pulsars? Which group would have a higher proportion of pulsars?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HvpvOqQQuER"
      },
      "source": [
        "# solution"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}